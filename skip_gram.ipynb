{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xineoh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "\n",
    "#os.chdir(os.path.dirname(os.path.realpath(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Declare model parameters\n",
    "batch_size = 50\n",
    "embedding_size = 200\n",
    "vocabulary_size = 10000\n",
    "generations = 50000\n",
    "print_loss_every = 500\n",
    "\n",
    "num_sampled = int(batch_size/2)    # Number of negative examples to sample.\n",
    "window_size = 2       # How many words to consider left and right.\n",
    "\n",
    "# Declare stop words\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# We pick five test words. We are expecting synonyms to appear\n",
    "print_valid_every = 2000\n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']\n",
    "# Later we will have to transform these into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load the movie review data\n",
    "# Check if data was downloaded, otherwise download it and save for future use\n",
    "def load_movie_data():\n",
    "    save_folder_name = 'temp'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polarity.neg')\n",
    "\n",
    "    # Check if files are already downloaded\n",
    "    if os.path.exists(save_folder_name):\n",
    "        pos_data = []\n",
    "        with open(pos_file, 'r') as temp_pos_file:\n",
    "            for row in temp_pos_file:\n",
    "                pos_data.append(row)\n",
    "        neg_data = []\n",
    "        with open(neg_file, 'r') as temp_neg_file:\n",
    "            for row in temp_neg_file:\n",
    "                neg_data.append(row)\n",
    "    else: # If not downloaded, download and save\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "        stream_data = urllib.request.urlopen(movie_data_url)\n",
    "        tmp = io.BytesIO()\n",
    "        while True:\n",
    "            s = stream_data.read(16384)\n",
    "            if not s:  \n",
    "                break\n",
    "            tmp.write(s)\n",
    "            stream_data.close()\n",
    "            tmp.seek(0)\n",
    "    \n",
    "        tar_file = tarfile.open(fileobj=tmp, mode=\"r:gz\")\n",
    "        pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
    "        neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
    "        # Save pos/neg reviews\n",
    "        pos_data = []\n",
    "        for line in pos:\n",
    "            pos_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "        neg_data = []\n",
    "        for line in neg:\n",
    "            neg_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "        tar_file.close()\n",
    "        # Write to file\n",
    "        if not os.path.exists(save_folder_name):\n",
    "            os.makedirs(save_folder_name)\n",
    "        # Save files\n",
    "        with open(pos_file, \"w\") as pos_file_handler:\n",
    "            pos_file_handler.write(''.join(pos_data))\n",
    "        with open(neg_file, \"w\") as neg_file_handler:\n",
    "            neg_file_handler.write(''.join(neg_data))\n",
    "    texts = pos_data + neg_data\n",
    "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
    "    return(texts, target)\n",
    "\n",
    "texts, target = load_movie_data()\n",
    "print(texts[0])\n",
    "print(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Normalize text\n",
    "def normalize_text(texts, stops):\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return(texts)\n",
    "    \n",
    "texts = normalize_text(texts, stops)\n",
    "\n",
    "# Texts must contain at least 3 words (each sentence contains more than 3 words)\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build dictionary of words\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    # Turn sentences (list of strings) into lists of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    \n",
    "    # Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = [['RARE', -1]]\n",
    "    \n",
    "    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    \n",
    "    # Now create the dictionary\n",
    "    word_dict = {}\n",
    "    # For each word, that we want in the dictionary, add it, then make it\n",
    "    # the value of the prior dictionary length\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)#################################################### word:index\n",
    "    \n",
    "    return(word_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text data into lists of integers from dictionary\n",
    "def text_to_numbers(sentences, word_dict):\n",
    "    # Initialize the returned data\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        # For each word, either use selected index or rare word index\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]  #counts\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)\n",
    "\n",
    "# Build our data set and dictionaries\n",
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "\n",
    "\n",
    "text_data = text_to_numbers(texts, word_dictionary)\n",
    "\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2700, 0, 1838, 1031, 0, 0, 721, 0, 0, 0, 2654, 721, 0, 0, 0, 0, 0, 1838, 721, 2654, 0, 1811, 2700, 0, 0, 0, 2654, 721, 2812, 0, 1838, 0, 2654, 0, 2654, 0, 2213, 721, 0, 0, 2674, 0, 0, 2654, 2674, 0, 0, 0, 1031, 721, 0, 0, 4993, 3941, 0, 0, 2213, 0, 721, 8449, 721, 2654, 0, 2674, 2700, 721, 0, 0, 721, 2700, 0, 0, 2700, 2654, 0, 3941, 0, 0, 0, 1838, 2213, 2812, 0, 2700, 0, 721, 2654, 721, 2674, 2674, 721, 2700, 0, 1113, 721, 0, 2654, 1838, 3941, 0, 1811, 0, 0, 8449, 0, 2654, 0, 0, 0, 0, 0, 721, 0, 0, 0, 721, 8449, 721, 2654, 0, 0, 721, 2674, 0, 3941]\n"
     ]
    }
   ],
   "source": [
    "print(text_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data randomly (N words behind, target, N words ahead)\n",
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentence to start\n",
    "\n",
    "        rand_sentence = np.random.choice(sentences) #return one sentence\n",
    "       # print(rand_sentence)\n",
    "        # Generate consecutive windows to look at\n",
    "\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        \n",
    "        # Denote which element of each window is the center word of interest\n",
    "\n",
    "        #print(\"window_sequences\",window_sequences)\n",
    "        \n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        #print(\"label_indices\",label_indices )\n",
    "        \n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            #print(\"batch_and_labels\",batch_and_labels)\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "            #print(\"tuple_data\",tuple_data[:2] )\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implmented yet.'.format(method))\n",
    "            \n",
    "        # extract batch and labels\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return(batch_data, label_data)\n",
    "for i in range(1):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)    \n",
    "    #print(\"batch_inputs\",batch_inputs )\n",
    "    #print(\"batch_labels\",batch_labels )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding:\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)\n",
    "\n",
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, y_target,  embed,num_sampled,vocabulary_size))\n",
    "                                     \n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims =True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500 : 50.77898406982422\n",
      "Loss at step 1000 : 15.63180923461914\n",
      "Loss at step 1500 : 12.383655548095703\n",
      "Loss at step 2000 : 3.6537389755249023\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 2500 : 9.307934761047363\n",
      "Loss at step 3000 : 28.51190948486328\n",
      "Loss at step 3500 : 2.0294852256774902\n",
      "Loss at step 4000 : 3.671635150909424\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 4500 : 12.236772537231445\n",
      "Loss at step 5000 : 4.977325916290283\n",
      "Loss at step 5500 : 1.431984305381775\n",
      "Loss at step 6000 : 1.6787762641906738\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 6500 : 4.984493255615234\n",
      "Loss at step 7000 : 1.567305088043213\n",
      "Loss at step 7500 : 2.8147671222686768\n",
      "Loss at step 8000 : 2.610617160797119\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 8500 : 2.9601142406463623\n",
      "Loss at step 9000 : 3.352146863937378\n",
      "Loss at step 9500 : 2.2441246509552\n",
      "Loss at step 10000 : 3.5827901363372803\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 10500 : 5.377118110656738\n",
      "Loss at step 11000 : 1.138452410697937\n",
      "Loss at step 11500 : 1.1660830974578857\n",
      "Loss at step 12000 : 1.3281962871551514\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 12500 : 2.423898458480835\n",
      "Loss at step 13000 : 3.9232850074768066\n",
      "Loss at step 13500 : 0.952239990234375\n",
      "Loss at step 14000 : 1.2825496196746826\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 14500 : 1.74277663230896\n",
      "Loss at step 15000 : 1.5411752462387085\n",
      "Loss at step 15500 : 2.0805575847625732\n",
      "Loss at step 16000 : 1.560849905014038\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 16500 : 1.7224901914596558\n",
      "Loss at step 17000 : 1.025576114654541\n",
      "Loss at step 17500 : 3.0527758598327637\n",
      "Loss at step 18000 : 4.3310112953186035\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 18500 : 2.4739389419555664\n",
      "Loss at step 19000 : 2.2136218547821045\n",
      "Loss at step 19500 : 1.7888286113739014\n",
      "Loss at step 20000 : 1.101355791091919\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 20500 : 1.8222835063934326\n",
      "Loss at step 21000 : 3.2745447158813477\n",
      "Loss at step 21500 : 1.879040241241455\n",
      "Loss at step 22000 : 2.055429458618164\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 22500 : 1.5944626331329346\n",
      "Loss at step 23000 : 2.8851077556610107\n",
      "Loss at step 23500 : 1.2657238245010376\n",
      "Loss at step 24000 : 2.15081524848938\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 24500 : 1.2920973300933838\n",
      "Loss at step 25000 : 1.1085690259933472\n",
      "Loss at step 25500 : 1.982008695602417\n",
      "Loss at step 26000 : 2.535475254058838\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 26500 : 0.8877705931663513\n",
      "Loss at step 27000 : 0.9577025771141052\n",
      "Loss at step 27500 : 2.025026559829712\n",
      "Loss at step 28000 : 2.4947614669799805\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 28500 : 1.5221482515335083\n",
      "Loss at step 29000 : 2.345292091369629\n",
      "Loss at step 29500 : 1.5950161218643188\n",
      "Loss at step 30000 : 0.06713483482599258\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 30500 : 1.054108738899231\n",
      "Loss at step 31000 : 2.0346853733062744\n",
      "Loss at step 31500 : 1.7112857103347778\n",
      "Loss at step 32000 : 2.742891311645508\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 32500 : 0.3276311457157135\n",
      "Loss at step 33000 : 1.0613336563110352\n",
      "Loss at step 33500 : 0.740650475025177\n",
      "Loss at step 34000 : 0.6560044288635254\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 34500 : 1.0263478755950928\n",
      "Loss at step 35000 : 1.5766000747680664\n",
      "Loss at step 35500 : 2.0771398544311523\n",
      "Loss at step 36000 : 2.193027973175049\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 36500 : 1.5933908224105835\n",
      "Loss at step 37000 : 2.2624058723449707\n",
      "Loss at step 37500 : 1.4665138721466064\n",
      "Loss at step 38000 : 2.9751980304718018\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 38500 : 1.4341135025024414\n",
      "Loss at step 39000 : 1.4316216707229614\n",
      "Loss at step 39500 : 1.3051886558532715\n",
      "Loss at step 40000 : 0.922391951084137\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 40500 : 1.9356412887573242\n",
      "Loss at step 41000 : 1.6585406064987183\n",
      "Loss at step 41500 : 1.2626125812530518\n",
      "Loss at step 42000 : 1.7260364294052124\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 42500 : 0.4796856641769409\n",
      "Loss at step 43000 : 0.9881803393363953\n",
      "Loss at step 43500 : 1.0754231214523315\n",
      "Loss at step 44000 : 0.9469048380851746\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 44500 : 2.792102575302124\n",
      "Loss at step 45000 : 1.81447172164917\n",
      "Loss at step 45500 : 1.3256930112838745\n",
      "Loss at step 46000 : 1.6030325889587402\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 46500 : 0.8455862998962402\n",
      "Loss at step 47000 : 2.2123799324035645\n",
      "Loss at step 47500 : 1.265244722366333\n",
      "Loss at step 48000 : 1.6436892747879028\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n",
      "Loss at step 48500 : 1.8258100748062134\n",
      "Loss at step 49000 : 3.215477228164673\n",
      "Loss at step 49500 : 1.12936270236969\n",
      "Loss at step 50000 : 0.8538681268692017\n",
      "Nearest to cliche: character, opened, powerpuff, phillip, adam,\n",
      "Nearest to love: warned, metropolitan, itll, strengths, hints,\n",
      "Nearest to hate: feardotcom, superbly, hanging, photographers, lint,\n",
      "Nearest to silly: indistinct, irrelevant, smith, tear, essential,\n",
      "Nearest to sad: perspective, shadyac, lump, performance, consolation,\n"
     ]
    }
   ],
   "source": [
    "# Run the skip gram model.\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
